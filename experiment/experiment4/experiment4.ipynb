{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2d9e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "import bs4\n",
    "import pandas as pd\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from textacy import extract\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import networkx as nx\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import os\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# assign directory\n",
    "directory = '../docs2'\n",
    "\n",
    "parsed_text = \"\"\n",
    "\n",
    "# iterate over files in\n",
    "# that directory\n",
    "# We will get a list of sentences that we will be processing\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        # creating a pdf file object\n",
    "        pdfFileObj = open(f, 'rb')\n",
    "\n",
    "        # creating a pdf reader object\n",
    "        pdfReader = PdfReader(pdfFileObj)\n",
    "\n",
    "        # printing number of pages in pdf file\n",
    "        print(len(pdfReader.pages))\n",
    "\n",
    "        # creating a page object\n",
    "        for page in pdfReader.pages:\n",
    "            # extracting text from page\n",
    "            parsed_text = \" \".join((parsed_text, str(page.extract_text())))\n",
    "\n",
    "            # closing the pdf file object\n",
    "        pdfFileObj.close()\n",
    "\n",
    "sentences = [[i] for i in nlp(parsed_text).sents]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "headers = ['sentence']\n",
    "values = sentences\n",
    "filename = 'doc_text.csv'\n",
    "with open(filename, 'w',newline='',encoding=\"utf-8\") as data:\n",
    "    writer = csv.writer(data)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "sentences = pd.read_csv(\"doc_text_clean.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b9b52586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root_of_sentence(doc):\n",
    "    root_token = None\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            root_token = token\n",
    "    return root_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "85061995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to find verb phrases, we will need to compile regular expression-like patterns for the part of speech combinations of the words that make up the verb phrase. If we print out parts of speech of verb phrases of the two preceding sentences, are made of and have, we will see that the part of speech sequences are AUX, VERB, ADP, and AUX.\n",
    "\n",
    "verb_patterns = [[{\"POS\": \"AUX\"}, {\"POS\": \"VERB\"},\n",
    "                  {\"POS\": \"ADP\"}],\n",
    "                 [{\"POS\": \"AUX\"}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f873b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The contains_root function checks if a verb phrase contains the root of the sentence:\n",
    "\n",
    "def contains_root(verb_phrase, root):\n",
    "    vp_start = verb_phrase.start\n",
    "    vp_end = verb_phrase.end\n",
    "    if vp_start <= root.i <= vp_end:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3a95c79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The get_verb_phrases function gets the verb phrases from a spaCy Doc object:\n",
    "\n",
    "def get_verb_phrases(doc):\n",
    "    root = find_root_of_sentence(doc)\n",
    "    verb_phrases = extract.token_matches(doc, verb_patterns)\n",
    "    new_vps = []\n",
    "    for verb_phrase in verb_phrases:\n",
    "        if contains_root(verb_phrase, root):\n",
    "            new_vps.append(verb_phrase)\n",
    "    return new_vps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c9ea9cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The find_noun_phrase function will look for noun phrases either on the left- or right-hand side of the main verb phrase:\n",
    "\n",
    "def find_noun_phrase(verb_phrase, noun_phrases, side):\n",
    "    for noun_phrase in noun_phrases:\n",
    "        if side == \"left\" and noun_phrase.start < verb_phrase.start:\n",
    "            return noun_phrase\n",
    "        elif side == \"right\" and noun_phrase.start > verb_phrase.start:\n",
    "            return noun_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a23c9fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The longer_verb_phrase function finds the longest verb phrase:\n",
    "\n",
    "def longer_verb_phrase(verb_phrases):\n",
    "    longest_length = 0\n",
    "    longest_verb_phrase = None\n",
    "    for verb_phrase in verb_phrases:\n",
    "        if len(verb_phrase) > longest_length:\n",
    "            longest_verb_phrase = verb_phrase\n",
    "    return longest_verb_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "#The find_noun_phrase function will look for noun phrases either on the left- or right-hand side of the main verb phrase:\n",
    "\n",
    "def find_triplet(in_sentence):\n",
    "    doc = nlp(in_sentence)\n",
    "    verb_phrases = get_verb_phrases(doc)\n",
    "    noun_phrases = doc.noun_chunks\n",
    "    verb_phrase = None\n",
    "    if len(verb_phrases) > 1:\n",
    "        verb_phrase = longer_verb_phrase(list(verb_phrases))\n",
    "    elif len(verb_phrases) == 1:\n",
    "        verb_phrase = verb_phrases[0]\n",
    "    if verb_phrase:\n",
    "        left_noun_phrase = find_noun_phrase(verb_phrase, noun_phrases, \"left\")\n",
    "        right_noun_phrase = find_noun_phrase(verb_phrase, noun_phrases, \"right\")\n",
    "        return (left_noun_phrase, verb_phrase,\n",
    "                right_noun_phrase)\n",
    "    return None, None, None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "11959c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Widely dispersed fragmented populations \t are \t a challenge\n",
      "The species \t is \t a very large terrestrial decapod\n",
      "the\n",
      "sizes \t are \t they\n",
      "One major challenge \t is \t imperfect detection\n",
      "Capture-mark-recapture(CMR) models \t are \t the gold standard\n",
      "data \t can \t the challenges\n",
      "the context \t may range from \t CMR data\n",
      "Animal Conservation /C15/C15(2023\n",
      "This \t is \t commercial and no modiﬁcations\n",
      "Integrated models \t can \t such disparate data\n",
      "Coconut crabs \t are \t the world ’s largest terrestrial arthropod\n",
      "Coconut\n",
      "crabs \t are \t natural predation\n",
      "a result \t been upgraded from \t ‘Data De ﬁcient\n",
      "Coconut crab populations \t are \t steep decline\n",
      "Pemba \t is \t an oceanic island\n",
      "The island \t is \t fertilesoil\n",
      "Pemba \t is dominated by \t small-scale farming\n",
      "Unguja \t is \t a Landbridge island\n",
      "Survey periods \t were \t the early part\n",
      "Sampling sites \t were \t identi ﬁed\n",
      "each crab \t was \t a\n",
      "bucket\n",
      "lastingthe length \t be \t identi ﬁed\n",
      "the Terms \t are governed by \t the applicable Creative Commons License\n",
      "We \t are \t con ﬁdentthat\n",
      "Crabs \t are \t little\n",
      "avian predation\n",
      "The main source \t is \t most islands\n",
      "Closure \t be violated by \t movement\n",
      "Several sampled sites \t been \t movement\n",
      "This \t is \t acommon problem\n",
      "the Terms \t are governed by \t the applicable Creative Commons License\n",
      " Accounting\n",
      "Buffering \t is \t individual movementon\n",
      "these buffers \t are \t an average home-range radius\n",
      "This \t is \t a different buffer\n",
      "the Terms \t are governed by \t the applicable Creative Commons License\n",
      " island\n",
      "Sampled\n",
      "areas \t were mapped out \t inGoogle Earth\n",
      "it \t would \t 16 pre-\n",
      "2018 surveys\n",
      "An all-N-mixturemodel \t would \t the information\n",
      "This ‘Null model \t be \t site-level covariates\n",
      "These observations \t are modelled as \t Bernoulli random variables\n",
      "potential different detec-tion parameters \t is omitted from \t thefollowing equation\n",
      "data aug-\n",
      "mentation \t are augmented up \t a large number\n",
      "We \t was \t low(Appendix S1\n",
      "Chumbe \t were performed in \t the ‘lodge\n",
      "area\n",
      "Neither assumption \t be \t the\n",
      "alternative\n",
      "no survey period \t is \t the present model\n",
      "This \t is \t modelling counts\n",
      "Body mass \t is \t a measure\n",
      "the Terms \t are governed by \t the applicable Creative Commons License\n",
      " convergence\n",
      "A link \t be found in \t the Data\n",
      "Availability statement\n",
      "Results \t were recaptured at \t Table S1\n",
      "Coconut crab density\n",
      "Estimates \t were \t the next site\n",
      "Uncertainty \t was \t high (widthof 90%\n",
      "The\n",
      "total \t was estimated at \t 6687.97 individuals\n",
      "Drivers \t was \t the presence\n",
      "Drivers \t were \t government protection\n",
      "the body mass \t was \t the strongest predictor\n",
      "coconut crabs \t have \t rapid population declines\n",
      "coconut crab populations \t have declined by \t 80%\n",
      "the\n",
      "Mozambique Channel \t are \t Juan de Nova Island\n",
      "Coconut crabs \t are \t Madagascar\n",
      "The species \t is \t Reunion\n",
      "Misali \t is \t a high-density population\n",
      "These numbers \t might \t population\n",
      "Animal Conservation /C15/C15(2023\n",
      "the Terms \t are governed by \t the applicable Creative Commons License\n",
      " ﬂuctuations\n",
      "abundance estimates \t are \t our estimate\n",
      "Studies \t have centred on \t the\n",
      "ecolodge\n",
      "Our abun-\n",
      "dance estimate \t is \t the 291 individuals\n",
      "it \t is \t this\n",
      "we \t could \t the number\n",
      "our low\n",
      "estimates \t be \t the choice\n",
      "few exceptions \t were \t <5 crabs\n",
      "Body mass \t were \t the four\n",
      "protected sites\n",
      "the degree \t could point to \t protected subpopulations\n",
      "Tourist lodges \t may \t local crab densities\n",
      "this \t be assessed in \t the future\n",
      "spite \t been leased for \t hotel development\n",
      "Other sites \t being leased for \t invest-ment\n",
      "We \t are \t this metapopulation\n",
      "estimate\n",
      "the same mean parameters \t may \t ﬂate differences\n",
      "This \t be \t differences\n",
      "a more robust approach todensity estimation \t is \t preferable spatial capture\n",
      "Models \t be \t SCR data\n",
      "some sites \t was \t much largerthan\n",
      "the\n",
      "objective \t be \t remaining subpopulations\n",
      "the Terms \t are governed by \t the applicable Creative Commons License\n",
      " data imbalance\n",
      "even single visit surveys \t be \t additional information\n",
      "Principles \t be \t optimal sam-\n",
      "the behaviour ofpopulations \t is \t conservationscience\n",
      "It \t is seen in \t the context\n",
      "thesefragmented populations \t will \t conser-\n",
      "vation plans\n"
     ]
    }
   ],
   "source": [
    "# We can now loop through our sentence list to find its relation triplets:\n",
    "\n",
    "for sentence in sentences[\"sentence\"]:\n",
    "    (left_np, vp, right_np) = find_triplet(sentence)\n",
    "    if left_np and vp and right_np:\n",
    "        print(left_np, \"\\t\", vp, \"\\t\", right_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0fa925bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data \t can \t the challenges\n",
      "the context \t may range from \t CMR data\n",
      "Animal Conservation /C15/C15(2023\n",
      "Integrated models \t can \t such disparate data\n",
      "it \t would \t 16 pre-\n",
      "2018 surveys\n",
      "An all-N-mixturemodel \t would \t the information\n",
      "These numbers \t might \t population\n",
      "Animal Conservation /C15/C15(2023\n",
      "we \t could \t the number\n",
      "the degree \t could point to \t protected subpopulations\n",
      "Tourist lodges \t may \t local crab densities\n",
      "the same mean parameters \t may \t ﬂate differences\n",
      "thesefragmented populations \t will \t conser-\n",
      "vation plans\n"
     ]
    }
   ],
   "source": [
    "# And this time excluding some verbs\n",
    "\n",
    "for sentence in sentences[\"sentence\"]:\n",
    "    (left_np, vp, right_np) = find_triplet(sentence)\n",
    "    if left_np and vp and right_np and not any(verb in vp.text for verb in ['is', 'are', 'was', 'be', 'were', 'have']):\n",
    "        print(left_np, \"\\t\", vp, \"\\t\", right_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
